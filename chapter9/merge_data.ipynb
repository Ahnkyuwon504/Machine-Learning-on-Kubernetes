{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f444d0f8-6172-4201-92ea-83c5f15bfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a34996d-d2d8-48e6-ae08-99e166707a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f\"\\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--conf spark.hadoop.fs.s3a.multipart.size=104857600 \\\n",
    "--packages org.apache.hadoop:hadoop-aws:3.2.0,org.postgresql:postgresql:42.3.3 \\\n",
    "--master spark://{os.environ['SPARK_CLUSTER']}:7077 pyspark-shell \"\n",
    "\n",
    "# Create the spark application\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Enrich flights data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899d425b-35fa-4176-87dd-6e42fd4f3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count:5000\n"
     ]
    }
   ],
   "source": [
    "keys = [\"year\", \"month\", \"day\", \"flight_number\"]\n",
    "\n",
    "df_flights = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://pg-flights-data:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"flights\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"numPartitions\", 5000) \\\n",
    "    .option(\"partitionColumn\", \"flight_number\") \\\n",
    "    .option(\"lowerBound\", 1)\\\n",
    "    .option(\"upperBound\", 10000)\\\n",
    "    .load()\n",
    "\n",
    "print(f\"Partition count:{df_flights.rdd.getNumPartitions()}\")\n",
    "\n",
    "df_airlines = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://airport-data/airlines.csv\")\n",
    "df_airports = spark.read\\\n",
    "                .options(delimiter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://airport-data/airports.csv\")\n",
    "\n",
    "#df_flights.printSchema()\n",
    "#df_airlines.printSchema()\n",
    "#df_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d4431b-faea-4c6b-8077-3da74deee9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_airlines = df_airlines.select([col(c).alias(\"AL_\"+c) for c in df_airlines.columns])\n",
    "df_o_airports = df_airports.select([col(c).alias(\"ORIG_\"+c) for c in df_airports.columns])\n",
    "df_d_airports = df_airports.select([col(c).alias(\"DEST_\"+c) for c in df_airports.columns])\n",
    "#df_airlines.printSchema()\n",
    "#df_o_airports.printSchema()\n",
    "#df_d_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3185158-19ed-44e2-bbad-327bc64e9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = df_flights\\\n",
    "    .join(broadcast(df_airlines), df_flights.airline == df_airlines.AL_IATA_CODE)\\\n",
    "    .join(broadcast(df_o_airports), df_flights.origin_airport == df_o_airports.ORIG_IATA_CODE)\\\n",
    "    .join(broadcast(df_d_airports), df_flights.destination_airport == df_d_airports.DEST_IATA_CODE)\n",
    "\n",
    "#df_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3bd726-26e9-447a-8f63-737642205222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flights.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ffaf2-53d5-41ef-863b-e389f6c47562",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = \"s3a://flights-data/flights\"\n",
    "\n",
    "df_flights.write.mode(\"overwrite\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .format(\"parquet\").save(output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea1e0f6-1e30-429d-87d6-0f8b80470ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e88c07-6d17-49f9-a8a3-90efd4fb1c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
